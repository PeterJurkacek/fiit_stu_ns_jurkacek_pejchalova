I1201 01:17:00.106297 139641327941440 logger.py:38] LOGGING START. Data are saving to logs_dir: /labs/logs/experiment_4, models_dir: /labs/models/experiment_4
I1201 01:17:00.107063 139641327941440 logger.py:39] log_file_name:file:///labs/logs/experiment_4/runtime.log.0eac5930e9d6.501.log.INFO.20191201-011700.96
I1201 01:17:00.107643 139641327941440 utils.py:103] Greyscale: False
I1201 01:17:00.145212 139641327941440 utils.py:49] dirs: ['R', 'O'], from: /labs/data/raw/test_dataset/TRAIN
I1201 01:17:00.146038 139641327941440 load_dataset.py:22] DataInfo => dir_path: /labs/data/raw/test_dataset/TRAIN, count: 42, cache_file_path: /labs/data/processed/experiment_4/test_datasetTRAIN.tfcache
I1201 01:17:00.176030 139641327941440 utils.py:49] dirs: ['R', 'O'], from: /labs/data/raw/test_dataset/TEST
I1201 01:17:00.176845 139641327941440 load_dataset.py:22] DataInfo => dir_path: /labs/data/raw/test_dataset/TEST, count: 23, cache_file_path: /labs/data/processed/experiment_4/test_datasetTEST.tfcache
I1201 01:17:00.177584 139641327941440 utils.py:108] Number of channels: 3
I1201 01:17:00.666688 139641327941440 load_dataset.py:105] batch_size:16
BatchInfo => Count of images in one batch:16
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
I1201 01:17:01.398553 139641327941440 load_dataset.py:105] batch_size:16
BatchInfo => Count of images in one batch:16
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([False,  True]) -> array(['O'], dtype='<U1')
One image Shape:  (224, 224, 3)
Label:  array([ True, False]) -> array(['R'], dtype='<U1')
I1201 01:17:01.898566 139641327941440 utils.py:72] steps_per_epoch: 2
I1201 01:17:01.899464 139641327941440 utils.py:73] total_num_of_samples: 42
I1201 01:17:01.900258 139641327941440 utils.py:74] batch_size: 16
I1201 01:17:01.901047 139641327941440 utils.py:72] steps_per_epoch: 1
I1201 01:17:01.901776 139641327941440 utils.py:73] total_num_of_samples: 23
I1201 01:17:01.902493 139641327941440 utils.py:74] batch_size: 16
I1201 01:17:01.903119 139641327941440 tunning.py:40] input_shape:(224, 224, 3)
I1201 01:17:01.904037 139641327941440 tunning.py:42] number_of_classes:2
I1201 01:17:01.905260 139641327941440 tunning.py:60] --- Running training session 1, 20
I1201 01:17:01.906256 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.17767502508788902, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:17:01.907104 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:17:01.911029 139641327941440 keras_models.py:35] index: 0
I1201 01:17:01.949257 139641327941440 keras_models.py:35] index: 1
I1201 01:17:02.055316 139641327941440 tunning.py:109] model.build()
I1201 01:17:02.106347 139641327941440 tunning.py:82] model.fit()
I1201 01:17:02.107260 139641327941440 tunning.py:83] run_id: 0
I1201 01:17:02.107969 139641327941440 tunning.py:84] epochs:2
I1201 01:17:02.108649 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:02.109312 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.8548846542835236
Average test accuracy:  0.5
I1201 01:17:06.688836 139641327941440 tunning.py:106] Model 0 saved to model_path: /labs/models/experiment_4/model_0.h5
I1201 01:17:06.708914 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:06.709953 139641327941440 tunning.py:122] run_id: 0
I1201 01:17:06.710524 139641327941440 tunning.py:123] steps: 23
I1201 01:17:07.382096 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_0.h5
I1201 01:17:07.382734 139641327941440 layer_utils.py:165] Model: "sequential"
I1201 01:17:07.383306 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:07.383807 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:07.384294 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:07.384975 139641327941440 layer_utils.py:163] conv2d (Conv2D)              (None, 224, 224, 8)       608       
I1201 01:17:07.385442 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.386038 139641327941440 layer_utils.py:163] max_pooling2d (MaxPooling2D) (None, 112, 112, 8)       0         
I1201 01:17:07.386483 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.387160 139641327941440 layer_utils.py:163] conv2d_1 (Conv2D)            (None, 112, 112, 16)      3216      
I1201 01:17:07.387608 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.388090 139641327941440 layer_utils.py:163] max_pooling2d_1 (MaxPooling2 (None, 56, 56, 16)        0         
I1201 01:17:07.388503 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.388966 139641327941440 layer_utils.py:163] flatten (Flatten)            (None, 50176)             0         
I1201 01:17:07.389377 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.389887 139641327941440 layer_utils.py:163] dropout (Dropout)            (None, 50176)             0         
I1201 01:17:07.390308 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.390846 139641327941440 layer_utils.py:163] dense (Dense)                (None, 32)                1605664   
I1201 01:17:07.391265 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:07.391812 139641327941440 layer_utils.py:163] dense_1 (Dense)              (None, 2)                 66        
I1201 01:17:07.392234 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:07.393059 139641327941440 layer_utils.py:242] Total params: 1,609,554
I1201 01:17:07.393476 139641327941440 layer_utils.py:243] Trainable params: 1,609,554
I1201 01:17:07.393869 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:07.394246 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.6956522
I1201 01:17:11.867138 139641327941440 tunning.py:60] --- Running training session 2, 20
I1201 01:17:11.868061 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.17767502508788902, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:17:11.868692 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:17:11.870231 139641327941440 keras_models.py:35] index: 0
I1201 01:17:11.904701 139641327941440 keras_models.py:35] index: 1
I1201 01:17:11.980462 139641327941440 tunning.py:109] model.build()
I1201 01:17:12.020017 139641327941440 tunning.py:82] model.fit()
I1201 01:17:12.020819 139641327941440 tunning.py:83] run_id: 1
I1201 01:17:12.021453 139641327941440 tunning.py:84] epochs:2
I1201 01:17:12.021992 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:12.022566 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.7605658769607544
Average test accuracy:  0.484375
I1201 01:17:16.245017 139641327941440 tunning.py:106] Model 1 saved to model_path: /labs/models/experiment_4/model_1.h5
I1201 01:17:16.257889 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:16.258556 139641327941440 tunning.py:122] run_id: 1
I1201 01:17:16.259247 139641327941440 tunning.py:123] steps: 23
I1201 01:17:16.893691 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_1.h5
I1201 01:17:16.894485 139641327941440 layer_utils.py:165] Model: "sequential_1"
I1201 01:17:16.895086 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:16.895676 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:16.896270 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:16.896982 139641327941440 layer_utils.py:163] conv2d_2 (Conv2D)            (None, 224, 224, 8)       608       
I1201 01:17:16.897629 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.898241 139641327941440 layer_utils.py:163] max_pooling2d_2 (MaxPooling2 (None, 112, 112, 8)       0         
I1201 01:17:16.898779 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.899374 139641327941440 layer_utils.py:163] conv2d_3 (Conv2D)            (None, 112, 112, 16)      3216      
I1201 01:17:16.900036 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.900651 139641327941440 layer_utils.py:163] max_pooling2d_3 (MaxPooling2 (None, 56, 56, 16)        0         
I1201 01:17:16.901216 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.901815 139641327941440 layer_utils.py:163] flatten_1 (Flatten)          (None, 50176)             0         
I1201 01:17:16.902264 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.902872 139641327941440 layer_utils.py:163] dropout_1 (Dropout)          (None, 50176)             0         
I1201 01:17:16.903327 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.903908 139641327941440 layer_utils.py:163] dense_2 (Dense)              (None, 32)                1605664   
I1201 01:17:16.904296 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:16.904864 139641327941440 layer_utils.py:163] dense_3 (Dense)              (None, 2)                 66        
I1201 01:17:16.905242 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:16.906135 139641327941440 layer_utils.py:242] Total params: 1,609,554
I1201 01:17:16.906637 139641327941440 layer_utils.py:243] Trainable params: 1,609,554
I1201 01:17:16.907090 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:16.907663 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:17:21.257851 139641327941440 tunning.py:60] --- Running training session 3, 20
I1201 01:17:21.258860 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.20741481240849652, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:17:21.259592 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:17:21.261302 139641327941440 keras_models.py:35] index: 0
I1201 01:17:21.291408 139641327941440 keras_models.py:35] index: 1
I1201 01:17:21.378488 139641327941440 tunning.py:109] model.build()
I1201 01:17:21.415550 139641327941440 tunning.py:82] model.fit()
I1201 01:17:21.416240 139641327941440 tunning.py:83] run_id: 2
I1201 01:17:21.416711 139641327941440 tunning.py:84] epochs:2
I1201 01:17:21.417133 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:21.417546 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.9807515442371368
Average test accuracy:  0.5
I1201 01:17:25.695095 139641327941440 tunning.py:106] Model 2 saved to model_path: /labs/models/experiment_4/model_2.h5
I1201 01:17:25.702977 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:25.703762 139641327941440 tunning.py:122] run_id: 2
I1201 01:17:25.704372 139641327941440 tunning.py:123] steps: 23
I1201 01:17:26.389390 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_2.h5
I1201 01:17:26.390112 139641327941440 layer_utils.py:165] Model: "sequential_2"
I1201 01:17:26.390751 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:26.391414 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:26.392038 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:26.392898 139641327941440 layer_utils.py:163] conv2d_4 (Conv2D)            (None, 224, 224, 8)       608       
I1201 01:17:26.393519 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.394223 139641327941440 layer_utils.py:163] max_pooling2d_4 (MaxPooling2 (None, 112, 112, 8)       0         
I1201 01:17:26.394690 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.395429 139641327941440 layer_utils.py:163] conv2d_5 (Conv2D)            (None, 112, 112, 16)      3216      
I1201 01:17:26.395898 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.396526 139641327941440 layer_utils.py:163] max_pooling2d_5 (MaxPooling2 (None, 56, 56, 16)        0         
I1201 01:17:26.397115 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.397797 139641327941440 layer_utils.py:163] flatten_2 (Flatten)          (None, 50176)             0         
I1201 01:17:26.398313 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.398842 139641327941440 layer_utils.py:163] dropout_2 (Dropout)          (None, 50176)             0         
I1201 01:17:26.399339 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.400044 139641327941440 layer_utils.py:163] dense_4 (Dense)              (None, 32)                1605664   
I1201 01:17:26.400495 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.401101 139641327941440 layer_utils.py:163] dense_5 (Dense)              (None, 64)                2112      
I1201 01:17:26.401569 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:26.402164 139641327941440 layer_utils.py:163] dense_6 (Dense)              (None, 2)                 130       
I1201 01:17:26.402639 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:26.403720 139641327941440 layer_utils.py:242] Total params: 1,611,730
I1201 01:17:26.404392 139641327941440 layer_utils.py:243] Trainable params: 1,611,730
I1201 01:17:26.404939 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:26.405392 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.8695652
I1201 01:17:30.831969 139641327941440 tunning.py:60] --- Running training session 4, 20
I1201 01:17:30.833026 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.20741481240849652, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:17:30.833688 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:17:30.834835 139641327941440 keras_models.py:35] index: 0
I1201 01:17:30.862519 139641327941440 keras_models.py:35] index: 1
I1201 01:17:30.948023 139641327941440 tunning.py:109] model.build()
I1201 01:17:31.087845 139641327941440 tunning.py:82] model.fit()
I1201 01:17:31.088544 139641327941440 tunning.py:83] run_id: 3
I1201 01:17:31.089052 139641327941440 tunning.py:84] epochs:2
I1201 01:17:31.089662 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:31.090178 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.5125692188739777
Average test accuracy:  0.5
I1201 01:17:35.186979 139641327941440 tunning.py:106] Model 3 saved to model_path: /labs/models/experiment_4/model_3.h5
I1201 01:17:35.193491 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:35.194009 139641327941440 tunning.py:122] run_id: 3
I1201 01:17:35.194526 139641327941440 tunning.py:123] steps: 23
I1201 01:17:35.875864 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_3.h5
I1201 01:17:35.876905 139641327941440 layer_utils.py:165] Model: "sequential_3"
I1201 01:17:35.877617 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:35.878270 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:35.878842 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:35.879733 139641327941440 layer_utils.py:163] conv2d_6 (Conv2D)            (None, 224, 224, 8)       608       
I1201 01:17:35.880449 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.881183 139641327941440 layer_utils.py:163] max_pooling2d_6 (MaxPooling2 (None, 112, 112, 8)       0         
I1201 01:17:35.881716 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.882439 139641327941440 layer_utils.py:163] conv2d_7 (Conv2D)            (None, 112, 112, 16)      3216      
I1201 01:17:35.883058 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.883661 139641327941440 layer_utils.py:163] max_pooling2d_7 (MaxPooling2 (None, 56, 56, 16)        0         
I1201 01:17:35.884406 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.885180 139641327941440 layer_utils.py:163] flatten_3 (Flatten)          (None, 50176)             0         
I1201 01:17:35.885865 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.886451 139641327941440 layer_utils.py:163] dropout_3 (Dropout)          (None, 50176)             0         
I1201 01:17:35.886982 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.887678 139641327941440 layer_utils.py:163] dense_7 (Dense)              (None, 32)                1605664   
I1201 01:17:35.888256 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.888955 139641327941440 layer_utils.py:163] dense_8 (Dense)              (None, 64)                2112      
I1201 01:17:35.889534 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:35.890290 139641327941440 layer_utils.py:163] dense_9 (Dense)              (None, 2)                 130       
I1201 01:17:35.890835 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:35.891931 139641327941440 layer_utils.py:242] Total params: 1,611,730
I1201 01:17:35.892522 139641327941440 layer_utils.py:243] Trainable params: 1,611,730
I1201 01:17:35.893052 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:35.893740 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:17:40.440944 139641327941440 tunning.py:60] --- Running training session 5, 20
I1201 01:17:40.441608 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.14192373549000367, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:17:40.442202 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:17:40.443256 139641327941440 keras_models.py:35] index: 0
I1201 01:17:40.475062 139641327941440 keras_models.py:35] index: 1
I1201 01:17:40.492667 139641327941440 keras_models.py:35] index: 2
I1201 01:17:40.575296 139641327941440 tunning.py:109] model.build()
I1201 01:17:40.612231 139641327941440 tunning.py:82] model.fit()
I1201 01:17:40.612999 139641327941440 tunning.py:83] run_id: 4
I1201 01:17:40.613566 139641327941440 tunning.py:84] epochs:2
I1201 01:17:40.614180 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:40.614758 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.8714583665132523
Average test accuracy:  0.515625
I1201 01:17:43.906785 139641327941440 tunning.py:106] Model 4 saved to model_path: /labs/models/experiment_4/model_4.h5
I1201 01:17:43.913821 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:43.914355 139641327941440 tunning.py:122] run_id: 4
I1201 01:17:43.914796 139641327941440 tunning.py:123] steps: 23
I1201 01:17:44.794417 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_4.h5
I1201 01:17:44.795105 139641327941440 layer_utils.py:165] Model: "sequential_4"
I1201 01:17:44.795678 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:44.796247 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:44.796762 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:44.797530 139641327941440 layer_utils.py:163] conv2d_8 (Conv2D)            (None, 224, 224, 8)       224       
I1201 01:17:44.798083 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.798658 139641327941440 layer_utils.py:163] max_pooling2d_8 (MaxPooling2 (None, 112, 112, 8)       0         
I1201 01:17:44.799128 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.799682 139641327941440 layer_utils.py:163] conv2d_9 (Conv2D)            (None, 112, 112, 16)      1168      
I1201 01:17:44.800146 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.800624 139641327941440 layer_utils.py:163] max_pooling2d_9 (MaxPooling2 (None, 56, 56, 16)        0         
I1201 01:17:44.801081 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.801635 139641327941440 layer_utils.py:163] conv2d_10 (Conv2D)           (None, 56, 56, 32)        4640      
I1201 01:17:44.802096 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.802582 139641327941440 layer_utils.py:163] max_pooling2d_10 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:17:44.803048 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.803526 139641327941440 layer_utils.py:163] flatten_4 (Flatten)          (None, 25088)             0         
I1201 01:17:44.803958 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.804443 139641327941440 layer_utils.py:163] dropout_4 (Dropout)          (None, 25088)             0         
I1201 01:17:44.804841 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.805405 139641327941440 layer_utils.py:163] dense_10 (Dense)             (None, 32)                802848    
I1201 01:17:44.806052 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.806655 139641327941440 layer_utils.py:163] dense_11 (Dense)             (None, 64)                2112      
I1201 01:17:44.807073 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:44.807631 139641327941440 layer_utils.py:163] dense_12 (Dense)             (None, 2)                 130       
I1201 01:17:44.808093 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:44.809100 139641327941440 layer_utils.py:242] Total params: 811,122
I1201 01:17:44.809606 139641327941440 layer_utils.py:243] Trainable params: 811,122
I1201 01:17:44.810013 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:44.810576 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:17:48.848038 139641327941440 tunning.py:60] --- Running training session 6, 20
I1201 01:17:48.848740 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.14192373549000367, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:17:48.849244 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:17:48.850362 139641327941440 keras_models.py:35] index: 0
I1201 01:17:48.884616 139641327941440 keras_models.py:35] index: 1
I1201 01:17:48.903721 139641327941440 keras_models.py:35] index: 2
I1201 01:17:48.986602 139641327941440 tunning.py:109] model.build()
I1201 01:17:49.027516 139641327941440 tunning.py:82] model.fit()
I1201 01:17:49.028262 139641327941440 tunning.py:83] run_id: 5
I1201 01:17:49.028843 139641327941440 tunning.py:84] epochs:2
I1201 01:17:49.029415 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:49.029921 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.8211830705404282
Average test accuracy:  0.453125
I1201 01:17:52.338277 139641327941440 tunning.py:106] Model 5 saved to model_path: /labs/models/experiment_4/model_5.h5
I1201 01:17:52.342830 139641327941440 tunning.py:121] model.evaluate()
I1201 01:17:52.343470 139641327941440 tunning.py:122] run_id: 5
I1201 01:17:52.344043 139641327941440 tunning.py:123] steps: 23
I1201 01:17:53.289969 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_5.h5
I1201 01:17:53.290739 139641327941440 layer_utils.py:165] Model: "sequential_5"
I1201 01:17:53.291352 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:17:53.291978 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:17:53.292652 139641327941440 layer_utils.py:168] =================================================================
I1201 01:17:53.293426 139641327941440 layer_utils.py:163] conv2d_11 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:17:53.294028 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.294681 139641327941440 layer_utils.py:163] max_pooling2d_11 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:17:53.295263 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.295993 139641327941440 layer_utils.py:163] conv2d_12 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:17:53.296611 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.297397 139641327941440 layer_utils.py:163] max_pooling2d_12 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:17:53.298128 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.298886 139641327941440 layer_utils.py:163] conv2d_13 (Conv2D)           (None, 56, 56, 32)        4640      
I1201 01:17:53.299498 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.300208 139641327941440 layer_utils.py:163] max_pooling2d_13 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:17:53.300798 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.301481 139641327941440 layer_utils.py:163] flatten_5 (Flatten)          (None, 25088)             0         
I1201 01:17:53.302074 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.302716 139641327941440 layer_utils.py:163] dropout_5 (Dropout)          (None, 25088)             0         
I1201 01:17:53.303249 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.303990 139641327941440 layer_utils.py:163] dense_13 (Dense)             (None, 32)                802848    
I1201 01:17:53.304665 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.305393 139641327941440 layer_utils.py:163] dense_14 (Dense)             (None, 64)                2112      
I1201 01:17:53.305980 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:17:53.307195 139641327941440 layer_utils.py:163] dense_15 (Dense)             (None, 2)                 130       
I1201 01:17:53.307880 139641327941440 layer_utils.py:230] =================================================================
I1201 01:17:53.309075 139641327941440 layer_utils.py:242] Total params: 811,122
I1201 01:17:53.309680 139641327941440 layer_utils.py:243] Trainable params: 811,122
I1201 01:17:53.310239 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:17:53.310889 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:17:57.037538 139641327941440 tunning.py:60] --- Running training session 7, 20
I1201 01:17:57.040045 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.394835642811296, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:17:57.042204 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:17:57.044432 139641327941440 keras_models.py:35] index: 0
I1201 01:17:57.065354 139641327941440 keras_models.py:35] index: 1
I1201 01:17:57.083374 139641327941440 keras_models.py:35] index: 2
I1201 01:17:57.182055 139641327941440 tunning.py:109] model.build()
I1201 01:17:57.220727 139641327941440 tunning.py:82] model.fit()
I1201 01:17:57.221512 139641327941440 tunning.py:83] run_id: 6
I1201 01:17:57.222079 139641327941440 tunning.py:84] epochs:2
I1201 01:17:57.222575 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:17:57.223081 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.7036029100418091
Average test accuracy:  0.484375
I1201 01:18:01.791509 139641327941440 tunning.py:106] Model 6 saved to model_path: /labs/models/experiment_4/model_6.h5
I1201 01:18:01.797515 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:01.798064 139641327941440 tunning.py:122] run_id: 6
I1201 01:18:01.798563 139641327941440 tunning.py:123] steps: 23
I1201 01:18:02.820002 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_6.h5
I1201 01:18:02.820776 139641327941440 layer_utils.py:165] Model: "sequential_6"
I1201 01:18:02.821551 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:02.822221 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:02.822817 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:02.823696 139641327941440 layer_utils.py:163] conv2d_14 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:18:02.824334 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.825018 139641327941440 layer_utils.py:163] max_pooling2d_14 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:02.825608 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.826377 139641327941440 layer_utils.py:163] conv2d_15 (Conv2D)           (None, 112, 112, 16)      3216      
I1201 01:18:02.826944 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.827486 139641327941440 layer_utils.py:163] max_pooling2d_15 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:02.827999 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.828767 139641327941440 layer_utils.py:163] conv2d_16 (Conv2D)           (None, 56, 56, 32)        12832     
I1201 01:18:02.829274 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.830005 139641327941440 layer_utils.py:163] max_pooling2d_16 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:18:02.830562 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.831070 139641327941440 layer_utils.py:163] flatten_6 (Flatten)          (None, 25088)             0         
I1201 01:18:02.831596 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.832123 139641327941440 layer_utils.py:163] dropout_6 (Dropout)          (None, 25088)             0         
I1201 01:18:02.832592 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.833193 139641327941440 layer_utils.py:163] dense_16 (Dense)             (None, 32)                802848    
I1201 01:18:02.833672 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.834266 139641327941440 layer_utils.py:163] dense_17 (Dense)             (None, 64)                2112      
I1201 01:18:02.834929 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.835589 139641327941440 layer_utils.py:163] dense_18 (Dense)             (None, 128)               8320      
I1201 01:18:02.836087 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:02.836702 139641327941440 layer_utils.py:163] dense_19 (Dense)             (None, 2)                 258       
I1201 01:18:02.837295 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:02.838613 139641327941440 layer_utils.py:242] Total params: 830,194
I1201 01:18:02.839244 139641327941440 layer_utils.py:243] Trainable params: 830,194
I1201 01:18:02.839760 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:02.840272 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.8695652
I1201 01:18:07.977927 139641327941440 tunning.py:60] --- Running training session 8, 20
I1201 01:18:07.978605 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.394835642811296, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:18:07.979330 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:18:07.983437 139641327941440 keras_models.py:35] index: 0
I1201 01:18:08.013395 139641327941440 keras_models.py:35] index: 1
I1201 01:18:08.038929 139641327941440 keras_models.py:35] index: 2
I1201 01:18:08.139890 139641327941440 tunning.py:109] model.build()
I1201 01:18:08.178556 139641327941440 tunning.py:82] model.fit()
I1201 01:18:08.179276 139641327941440 tunning.py:83] run_id: 7
I1201 01:18:08.179879 139641327941440 tunning.py:84] epochs:2
I1201 01:18:08.180517 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:08.181027 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.6986623555421829
Average test accuracy:  0.5625
I1201 01:18:12.931921 139641327941440 tunning.py:106] Model 7 saved to model_path: /labs/models/experiment_4/model_7.h5
I1201 01:18:12.939890 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:12.940663 139641327941440 tunning.py:122] run_id: 7
I1201 01:18:12.941375 139641327941440 tunning.py:123] steps: 23
I1201 01:18:13.946258 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_7.h5
I1201 01:18:13.946946 139641327941440 layer_utils.py:165] Model: "sequential_7"
I1201 01:18:13.947611 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:13.948206 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:13.948786 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:13.949554 139641327941440 layer_utils.py:163] conv2d_17 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:18:13.950126 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.950799 139641327941440 layer_utils.py:163] max_pooling2d_17 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:13.951383 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.952113 139641327941440 layer_utils.py:163] conv2d_18 (Conv2D)           (None, 112, 112, 16)      3216      
I1201 01:18:13.952783 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.953415 139641327941440 layer_utils.py:163] max_pooling2d_18 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:13.953971 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.954665 139641327941440 layer_utils.py:163] conv2d_19 (Conv2D)           (None, 56, 56, 32)        12832     
I1201 01:18:13.955222 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.955795 139641327941440 layer_utils.py:163] max_pooling2d_19 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:18:13.956314 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.956866 139641327941440 layer_utils.py:163] flatten_7 (Flatten)          (None, 25088)             0         
I1201 01:18:13.957388 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.961077 139641327941440 layer_utils.py:163] dropout_7 (Dropout)          (None, 25088)             0         
I1201 01:18:13.961692 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.962424 139641327941440 layer_utils.py:163] dense_20 (Dense)             (None, 32)                802848    
I1201 01:18:13.963073 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.963776 139641327941440 layer_utils.py:163] dense_21 (Dense)             (None, 64)                2112      
I1201 01:18:13.964329 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.964939 139641327941440 layer_utils.py:163] dense_22 (Dense)             (None, 128)               8320      
I1201 01:18:13.965482 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:13.966174 139641327941440 layer_utils.py:163] dense_23 (Dense)             (None, 2)                 258       
I1201 01:18:13.966752 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:13.967831 139641327941440 layer_utils.py:242] Total params: 830,194
I1201 01:18:13.968397 139641327941440 layer_utils.py:243] Trainable params: 830,194
I1201 01:18:13.968855 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:13.969318 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:18:18.956861 139641327941440 tunning.py:60] --- Running training session 9, 20
I1201 01:18:18.958121 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.12212273008861754, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:18:18.958999 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:18:18.960309 139641327941440 keras_models.py:35] index: 0
I1201 01:18:18.988726 139641327941440 keras_models.py:35] index: 1
I1201 01:18:19.092016 139641327941440 tunning.py:109] model.build()
I1201 01:18:19.129343 139641327941440 tunning.py:82] model.fit()
I1201 01:18:19.130124 139641327941440 tunning.py:83] run_id: 8
I1201 01:18:19.130836 139641327941440 tunning.py:84] epochs:2
I1201 01:18:19.131391 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:19.131942 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.8249850869178772
Average test accuracy:  0.484375
I1201 01:18:22.323942 139641327941440 tunning.py:106] Model 8 saved to model_path: /labs/models/experiment_4/model_8.h5
I1201 01:18:22.343639 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:22.344292 139641327941440 tunning.py:122] run_id: 8
I1201 01:18:22.344814 139641327941440 tunning.py:123] steps: 23
I1201 01:18:23.181060 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_8.h5
I1201 01:18:23.181695 139641327941440 layer_utils.py:165] Model: "sequential_8"
I1201 01:18:23.182190 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:23.182717 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:23.184045 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:23.184880 139641327941440 layer_utils.py:163] conv2d_20 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:18:23.185392 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.185872 139641327941440 layer_utils.py:163] max_pooling2d_20 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:23.186359 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.187002 139641327941440 layer_utils.py:163] conv2d_21 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:18:23.187502 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.187973 139641327941440 layer_utils.py:163] max_pooling2d_21 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:23.188395 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.188945 139641327941440 layer_utils.py:163] flatten_8 (Flatten)          (None, 50176)             0         
I1201 01:18:23.189451 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.190234 139641327941440 layer_utils.py:163] dropout_8 (Dropout)          (None, 50176)             0         
I1201 01:18:23.190712 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.191270 139641327941440 layer_utils.py:163] dense_24 (Dense)             (None, 32)                1605664   
I1201 01:18:23.191740 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.192303 139641327941440 layer_utils.py:163] dense_25 (Dense)             (None, 64)                2112      
I1201 01:18:23.192744 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.193302 139641327941440 layer_utils.py:163] dense_26 (Dense)             (None, 128)               8320      
I1201 01:18:23.193732 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:23.194297 139641327941440 layer_utils.py:163] dense_27 (Dense)             (None, 2)                 258       
I1201 01:18:23.194729 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:23.195729 139641327941440 layer_utils.py:242] Total params: 1,617,746
I1201 01:18:23.196185 139641327941440 layer_utils.py:243] Trainable params: 1,617,746
I1201 01:18:23.196597 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:23.197012 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:18:26.752223 139641327941440 tunning.py:60] --- Running training session 10, 20
I1201 01:18:26.752946 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.12212273008861754, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:18:26.754048 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:18:26.755619 139641327941440 keras_models.py:35] index: 0
I1201 01:18:26.788550 139641327941440 keras_models.py:35] index: 1
I1201 01:18:26.893231 139641327941440 tunning.py:109] model.build()
I1201 01:18:26.931998 139641327941440 tunning.py:82] model.fit()
I1201 01:18:26.932813 139641327941440 tunning.py:83] run_id: 9
I1201 01:18:26.933578 139641327941440 tunning.py:84] epochs:2
I1201 01:18:26.934316 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:26.934908 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.9124380648136139
Average test accuracy:  0.625
I1201 01:18:30.428140 139641327941440 tunning.py:106] Model 9 saved to model_path: /labs/models/experiment_4/model_9.h5
I1201 01:18:30.446367 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:30.447048 139641327941440 tunning.py:122] run_id: 9
I1201 01:18:30.447731 139641327941440 tunning.py:123] steps: 23
I1201 01:18:31.246797 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_9.h5
I1201 01:18:31.250354 139641327941440 layer_utils.py:165] Model: "sequential_9"
I1201 01:18:31.251179 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:31.251765 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:31.252358 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:31.253156 139641327941440 layer_utils.py:163] conv2d_22 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:18:31.253779 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.254391 139641327941440 layer_utils.py:163] max_pooling2d_22 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:31.254861 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.255679 139641327941440 layer_utils.py:163] conv2d_23 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:18:31.256326 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.256979 139641327941440 layer_utils.py:163] max_pooling2d_23 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:31.257451 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.257948 139641327941440 layer_utils.py:163] flatten_9 (Flatten)          (None, 50176)             0         
I1201 01:18:31.258428 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.259043 139641327941440 layer_utils.py:163] dropout_9 (Dropout)          (None, 50176)             0         
I1201 01:18:31.259527 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.260298 139641327941440 layer_utils.py:163] dense_28 (Dense)             (None, 32)                1605664   
I1201 01:18:31.260881 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.261644 139641327941440 layer_utils.py:163] dense_29 (Dense)             (None, 64)                2112      
I1201 01:18:31.262184 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.262825 139641327941440 layer_utils.py:163] dense_30 (Dense)             (None, 128)               8320      
I1201 01:18:31.263480 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:31.264185 139641327941440 layer_utils.py:163] dense_31 (Dense)             (None, 2)                 258       
I1201 01:18:31.264680 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:31.265736 139641327941440 layer_utils.py:242] Total params: 1,617,746
I1201 01:18:31.266372 139641327941440 layer_utils.py:243] Trainable params: 1,617,746
I1201 01:18:31.266836 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:31.267292 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:18:35.277436 139641327941440 tunning.py:60] --- Running training session 11, 20
I1201 01:18:35.278081 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.23025155063613512, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:18:35.278589 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:18:35.280151 139641327941440 keras_models.py:35] index: 0
I1201 01:18:35.311069 139641327941440 keras_models.py:35] index: 1
I1201 01:18:35.396427 139641327941440 tunning.py:109] model.build()
I1201 01:18:35.432823 139641327941440 tunning.py:82] model.fit()
I1201 01:18:35.433412 139641327941440 tunning.py:83] run_id: 10
I1201 01:18:35.434014 139641327941440 tunning.py:84] epochs:2
I1201 01:18:35.434606 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:35.435199 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.896891862154007
Average test accuracy:  0.5
I1201 01:18:39.039376 139641327941440 tunning.py:106] Model 10 saved to model_path: /labs/models/experiment_4/model_10.h5
I1201 01:18:39.044526 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:39.045345 139641327941440 tunning.py:122] run_id: 10
I1201 01:18:39.045961 139641327941440 tunning.py:123] steps: 23
I1201 01:18:39.812071 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_10.h5
I1201 01:18:39.812901 139641327941440 layer_utils.py:165] Model: "sequential_10"
I1201 01:18:39.813547 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:39.814225 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:39.814749 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:39.815635 139641327941440 layer_utils.py:163] conv2d_24 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:18:39.816314 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.817025 139641327941440 layer_utils.py:163] max_pooling2d_24 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:39.817637 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.818425 139641327941440 layer_utils.py:163] conv2d_25 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:18:39.819159 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.819829 139641327941440 layer_utils.py:163] max_pooling2d_25 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:39.820419 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.820978 139641327941440 layer_utils.py:163] flatten_10 (Flatten)         (None, 50176)             0         
I1201 01:18:39.821520 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.822105 139641327941440 layer_utils.py:163] dropout_10 (Dropout)         (None, 50176)             0         
I1201 01:18:39.822598 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.823227 139641327941440 layer_utils.py:163] dense_32 (Dense)             (None, 32)                1605664   
I1201 01:18:39.823828 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.824471 139641327941440 layer_utils.py:163] dense_33 (Dense)             (None, 64)                2112      
I1201 01:18:39.824976 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:39.825592 139641327941440 layer_utils.py:163] dense_34 (Dense)             (None, 2)                 130       
I1201 01:18:39.826099 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:39.827234 139641327941440 layer_utils.py:242] Total params: 1,609,298
I1201 01:18:39.827834 139641327941440 layer_utils.py:243] Trainable params: 1,609,298
I1201 01:18:39.828338 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:39.828876 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.6956522
I1201 01:18:43.547667 139641327941440 tunning.py:60] --- Running training session 12, 20
I1201 01:18:43.548413 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.23025155063613512, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:18:43.549078 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:18:43.550451 139641327941440 keras_models.py:35] index: 0
I1201 01:18:43.580446 139641327941440 keras_models.py:35] index: 1
I1201 01:18:43.670347 139641327941440 tunning.py:109] model.build()
I1201 01:18:43.707884 139641327941440 tunning.py:82] model.fit()
I1201 01:18:43.708846 139641327941440 tunning.py:83] run_id: 11
I1201 01:18:43.709483 139641327941440 tunning.py:84] epochs:2
I1201 01:18:43.710078 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:43.710599 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.7936423867940903
Average test accuracy:  0.484375
I1201 01:18:46.981966 139641327941440 tunning.py:106] Model 11 saved to model_path: /labs/models/experiment_4/model_11.h5
I1201 01:18:46.987131 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:46.988010 139641327941440 tunning.py:122] run_id: 11
I1201 01:18:46.988733 139641327941440 tunning.py:123] steps: 23
I1201 01:18:47.744101 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_11.h5
I1201 01:18:47.744877 139641327941440 layer_utils.py:165] Model: "sequential_11"
I1201 01:18:47.745588 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:47.746290 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:47.746924 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:47.747710 139641327941440 layer_utils.py:163] conv2d_26 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:18:47.748438 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.749122 139641327941440 layer_utils.py:163] max_pooling2d_26 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:47.749790 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.750620 139641327941440 layer_utils.py:163] conv2d_27 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:18:47.751240 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.751930 139641327941440 layer_utils.py:163] max_pooling2d_27 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:47.752485 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.753130 139641327941440 layer_utils.py:163] flatten_11 (Flatten)         (None, 50176)             0         
I1201 01:18:47.753754 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.754425 139641327941440 layer_utils.py:163] dropout_11 (Dropout)         (None, 50176)             0         
I1201 01:18:47.754969 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.755644 139641327941440 layer_utils.py:163] dense_35 (Dense)             (None, 32)                1605664   
I1201 01:18:47.756251 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.756890 139641327941440 layer_utils.py:163] dense_36 (Dense)             (None, 64)                2112      
I1201 01:18:47.757444 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:47.758143 139641327941440 layer_utils.py:163] dense_37 (Dense)             (None, 2)                 130       
I1201 01:18:47.758697 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:47.759752 139641327941440 layer_utils.py:242] Total params: 1,609,298
I1201 01:18:47.760622 139641327941440 layer_utils.py:243] Trainable params: 1,609,298
I1201 01:18:47.761244 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:47.761838 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:18:51.567793 139641327941440 tunning.py:60] --- Running training session 13, 20
I1201 01:18:51.568321 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.35959297833149206, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:18:51.569666 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:18:51.576012 139641327941440 keras_models.py:35] index: 0
I1201 01:18:51.597264 139641327941440 keras_models.py:35] index: 1
I1201 01:18:51.615354 139641327941440 keras_models.py:35] index: 2
I1201 01:18:51.716140 139641327941440 tunning.py:109] model.build()
I1201 01:18:51.754159 139641327941440 tunning.py:82] model.fit()
I1201 01:18:51.754910 139641327941440 tunning.py:83] run_id: 12
I1201 01:18:51.755434 139641327941440 tunning.py:84] epochs:2
I1201 01:18:51.755954 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:18:51.756470 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.58107940107584
Average test accuracy:  0.703125
I1201 01:18:56.291573 139641327941440 tunning.py:106] Model 12 saved to model_path: /labs/models/experiment_4/model_12.h5
I1201 01:18:56.298672 139641327941440 tunning.py:121] model.evaluate()
I1201 01:18:56.299302 139641327941440 tunning.py:122] run_id: 12
I1201 01:18:56.299892 139641327941440 tunning.py:123] steps: 23
I1201 01:18:57.083815 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_12.h5
I1201 01:18:57.084520 139641327941440 layer_utils.py:165] Model: "sequential_12"
I1201 01:18:57.085098 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:18:57.085689 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:18:57.086274 139641327941440 layer_utils.py:168] =================================================================
I1201 01:18:57.087049 139641327941440 layer_utils.py:163] conv2d_28 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:18:57.087665 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.088273 139641327941440 layer_utils.py:163] max_pooling2d_28 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:18:57.088800 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.089471 139641327941440 layer_utils.py:163] conv2d_29 (Conv2D)           (None, 112, 112, 16)      3216      
I1201 01:18:57.090018 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.090627 139641327941440 layer_utils.py:163] max_pooling2d_29 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:18:57.091146 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.091816 139641327941440 layer_utils.py:163] conv2d_30 (Conv2D)           (None, 56, 56, 32)        12832     
I1201 01:18:57.092363 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.093236 139641327941440 layer_utils.py:163] max_pooling2d_30 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:18:57.093801 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.094371 139641327941440 layer_utils.py:163] flatten_12 (Flatten)         (None, 25088)             0         
I1201 01:18:57.094870 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.095435 139641327941440 layer_utils.py:163] dropout_12 (Dropout)         (None, 25088)             0         
I1201 01:18:57.095973 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.096770 139641327941440 layer_utils.py:163] dense_38 (Dense)             (None, 32)                802848    
I1201 01:18:57.097356 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.097998 139641327941440 layer_utils.py:163] dense_39 (Dense)             (None, 64)                2112      
I1201 01:18:57.098545 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:18:57.099166 139641327941440 layer_utils.py:163] dense_40 (Dense)             (None, 2)                 130       
I1201 01:18:57.099711 139641327941440 layer_utils.py:230] =================================================================
I1201 01:18:57.100852 139641327941440 layer_utils.py:242] Total params: 821,746
I1201 01:18:57.101469 139641327941440 layer_utils.py:243] Trainable params: 821,746
I1201 01:18:57.101926 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:18:57.102375 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.9130435
I1201 01:19:02.169457 139641327941440 tunning.py:60] --- Running training session 14, 20
I1201 01:19:02.172266 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 2, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.35959297833149206, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:19:02.174978 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:19:02.177738 139641327941440 keras_models.py:35] index: 0
I1201 01:19:02.201488 139641327941440 keras_models.py:35] index: 1
I1201 01:19:02.220509 139641327941440 keras_models.py:35] index: 2
I1201 01:19:02.304226 139641327941440 tunning.py:109] model.build()
I1201 01:19:02.344524 139641327941440 tunning.py:82] model.fit()
I1201 01:19:02.345145 139641327941440 tunning.py:83] run_id: 13
I1201 01:19:02.345603 139641327941440 tunning.py:84] epochs:2
I1201 01:19:02.346037 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:02.346471 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.0817523300647736
Average test accuracy:  0.421875
I1201 01:19:06.971087 139641327941440 tunning.py:106] Model 13 saved to model_path: /labs/models/experiment_4/model_13.h5
I1201 01:19:06.978933 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:06.979759 139641327941440 tunning.py:122] run_id: 13
I1201 01:19:06.980548 139641327941440 tunning.py:123] steps: 23
I1201 01:19:07.996795 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_13.h5
I1201 01:19:07.997716 139641327941440 layer_utils.py:165] Model: "sequential_13"
I1201 01:19:07.998413 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:07.999153 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:07.999876 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:08.000706 139641327941440 layer_utils.py:163] conv2d_31 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:19:08.001274 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.001915 139641327941440 layer_utils.py:163] max_pooling2d_31 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:08.002480 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.003226 139641327941440 layer_utils.py:163] conv2d_32 (Conv2D)           (None, 112, 112, 16)      3216      
I1201 01:19:08.003834 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.004359 139641327941440 layer_utils.py:163] max_pooling2d_32 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:19:08.004887 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.005666 139641327941440 layer_utils.py:163] conv2d_33 (Conv2D)           (None, 56, 56, 32)        12832     
I1201 01:19:08.006237 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.006802 139641327941440 layer_utils.py:163] max_pooling2d_33 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:19:08.007574 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.008253 139641327941440 layer_utils.py:163] flatten_13 (Flatten)         (None, 25088)             0         
I1201 01:19:08.008821 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.009378 139641327941440 layer_utils.py:163] dropout_13 (Dropout)         (None, 25088)             0         
I1201 01:19:08.009896 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.010605 139641327941440 layer_utils.py:163] dense_41 (Dense)             (None, 32)                802848    
I1201 01:19:08.011216 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.011931 139641327941440 layer_utils.py:163] dense_42 (Dense)             (None, 64)                2112      
I1201 01:19:08.012588 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:08.013319 139641327941440 layer_utils.py:163] dense_43 (Dense)             (None, 2)                 130       
I1201 01:19:08.013915 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:08.015003 139641327941440 layer_utils.py:242] Total params: 821,746
I1201 01:19:08.015628 139641327941440 layer_utils.py:243] Trainable params: 821,746
I1201 01:19:08.016487 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:08.017069 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:19:13.949473 139641327941440 tunning.py:60] --- Running training session 15, 20
I1201 01:19:13.951725 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.3159114059211863, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:19:13.952979 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:19:13.955121 139641327941440 keras_models.py:35] index: 0
I1201 01:19:14.066872 139641327941440 tunning.py:109] model.build()
I1201 01:19:14.103689 139641327941440 tunning.py:82] model.fit()
I1201 01:19:14.104378 139641327941440 tunning.py:83] run_id: 14
I1201 01:19:14.104900 139641327941440 tunning.py:84] epochs:2
I1201 01:19:14.105373 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:14.105865 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  9.776996850967407
Average test accuracy:  0.5
I1201 01:19:17.435127 139641327941440 tunning.py:106] Model 14 saved to model_path: /labs/models/experiment_4/model_14.h5
I1201 01:19:17.495235 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:17.496395 139641327941440 tunning.py:122] run_id: 14
I1201 01:19:17.497017 139641327941440 tunning.py:123] steps: 23
I1201 01:19:18.290412 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_14.h5
I1201 01:19:18.291124 139641327941440 layer_utils.py:165] Model: "sequential_14"
I1201 01:19:18.291712 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:18.292305 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:18.292862 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:18.293639 139641327941440 layer_utils.py:163] conv2d_34 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:19:18.294205 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:18.294826 139641327941440 layer_utils.py:163] max_pooling2d_34 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:18.295371 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:18.295922 139641327941440 layer_utils.py:163] flatten_14 (Flatten)         (None, 100352)            0         
I1201 01:19:18.296526 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:18.297143 139641327941440 layer_utils.py:163] dropout_14 (Dropout)         (None, 100352)            0         
I1201 01:19:18.297626 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:18.298375 139641327941440 layer_utils.py:163] dense_44 (Dense)             (None, 32)                3211296   
I1201 01:19:18.298950 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:18.299644 139641327941440 layer_utils.py:163] dense_45 (Dense)             (None, 2)                 66        
I1201 01:19:18.300185 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:18.301295 139641327941440 layer_utils.py:242] Total params: 3,211,586
I1201 01:19:18.301888 139641327941440 layer_utils.py:243] Trainable params: 3,211,586
I1201 01:19:18.302492 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:18.302917 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.47826087
I1201 01:19:21.756956 139641327941440 tunning.py:60] --- Running training session 16, 20
I1201 01:19:21.757895 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.3159114059211863, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:19:21.758819 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:19:21.760246 139641327941440 keras_models.py:35] index: 0
I1201 01:19:21.861076 139641327941440 tunning.py:109] model.build()
I1201 01:19:21.899318 139641327941440 tunning.py:82] model.fit()
I1201 01:19:21.899963 139641327941440 tunning.py:83] run_id: 15
I1201 01:19:21.900439 139641327941440 tunning.py:84] epochs:2
I1201 01:19:21.900900 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:21.902623 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  4.642941355705261
Average test accuracy:  0.578125
I1201 01:19:25.464921 139641327941440 tunning.py:106] Model 15 saved to model_path: /labs/models/experiment_4/model_15.h5
I1201 01:19:25.521817 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:25.522628 139641327941440 tunning.py:122] run_id: 15
I1201 01:19:25.523548 139641327941440 tunning.py:123] steps: 23
I1201 01:19:26.639173 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_15.h5
I1201 01:19:26.659284 139641327941440 layer_utils.py:165] Model: "sequential_15"
I1201 01:19:26.660285 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:26.660954 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:26.661554 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:26.662412 139641327941440 layer_utils.py:163] conv2d_35 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:19:26.663128 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:26.663813 139641327941440 layer_utils.py:163] max_pooling2d_35 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:26.664390 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:26.665062 139641327941440 layer_utils.py:163] flatten_15 (Flatten)         (None, 100352)            0         
I1201 01:19:26.665668 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:26.666332 139641327941440 layer_utils.py:163] dropout_15 (Dropout)         (None, 100352)            0         
I1201 01:19:26.667091 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:26.667981 139641327941440 layer_utils.py:163] dense_46 (Dense)             (None, 32)                3211296   
I1201 01:19:26.668604 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:26.669363 139641327941440 layer_utils.py:163] dense_47 (Dense)             (None, 2)                 66        
I1201 01:19:26.670043 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:26.671187 139641327941440 layer_utils.py:242] Total params: 3,211,586
I1201 01:19:26.671951 139641327941440 layer_utils.py:243] Trainable params: 3,211,586
I1201 01:19:26.672625 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:26.676412 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5652174
I1201 01:19:30.017244 139641327941440 tunning.py:60] --- Running training session 17, 20
I1201 01:19:30.017910 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.24807335993959742, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:19:30.021735 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:19:30.023297 139641327941440 keras_models.py:35] index: 0
I1201 01:19:30.062283 139641327941440 keras_models.py:35] index: 1
I1201 01:19:30.081445 139641327941440 keras_models.py:35] index: 2
I1201 01:19:30.178873 139641327941440 tunning.py:109] model.build()
I1201 01:19:30.218717 139641327941440 tunning.py:82] model.fit()
I1201 01:19:30.219460 139641327941440 tunning.py:83] run_id: 16
I1201 01:19:30.220077 139641327941440 tunning.py:84] epochs:2
I1201 01:19:30.220653 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:30.221219 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.7039267122745514
Average test accuracy:  0.453125
I1201 01:19:33.813885 139641327941440 tunning.py:106] Model 16 saved to model_path: /labs/models/experiment_4/model_16.h5
I1201 01:19:33.820553 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:33.821101 139641327941440 tunning.py:122] run_id: 16
I1201 01:19:33.821632 139641327941440 tunning.py:123] steps: 23
I1201 01:19:34.715829 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_16.h5
I1201 01:19:34.716474 139641327941440 layer_utils.py:165] Model: "sequential_16"
I1201 01:19:34.717064 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:34.717656 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:34.718227 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:34.719026 139641327941440 layer_utils.py:163] conv2d_36 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:19:34.719604 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.720198 139641327941440 layer_utils.py:163] max_pooling2d_36 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:34.720704 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.721282 139641327941440 layer_utils.py:163] conv2d_37 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:19:34.721883 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.722544 139641327941440 layer_utils.py:163] max_pooling2d_37 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:19:34.723120 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.723719 139641327941440 layer_utils.py:163] conv2d_38 (Conv2D)           (None, 56, 56, 32)        4640      
I1201 01:19:34.724156 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.724681 139641327941440 layer_utils.py:163] max_pooling2d_38 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:19:34.725143 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.725607 139641327941440 layer_utils.py:163] flatten_16 (Flatten)         (None, 25088)             0         
I1201 01:19:34.726023 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.726511 139641327941440 layer_utils.py:163] dropout_16 (Dropout)         (None, 25088)             0         
I1201 01:19:34.726898 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.727414 139641327941440 layer_utils.py:163] dense_48 (Dense)             (None, 32)                802848    
I1201 01:19:34.727821 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.728336 139641327941440 layer_utils.py:163] dense_49 (Dense)             (None, 64)                2112      
I1201 01:19:34.728787 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.729286 139641327941440 layer_utils.py:163] dense_50 (Dense)             (None, 128)               8320      
I1201 01:19:34.729794 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:34.730369 139641327941440 layer_utils.py:163] dense_51 (Dense)             (None, 2)                 258       
I1201 01:19:34.730776 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:34.731802 139641327941440 layer_utils.py:242] Total params: 819,570
I1201 01:19:34.732229 139641327941440 layer_utils.py:243] Trainable params: 819,570
I1201 01:19:34.732606 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:34.733012 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.6086956
I1201 01:19:38.580209 139641327941440 tunning.py:60] --- Running training session 18, 20
I1201 01:19:38.581668 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 3, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.24807335993959742, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adam'}
I1201 01:19:38.582434 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:19:38.585086 139641327941440 keras_models.py:35] index: 0
I1201 01:19:38.611496 139641327941440 keras_models.py:35] index: 1
I1201 01:19:38.629979 139641327941440 keras_models.py:35] index: 2
I1201 01:19:38.730468 139641327941440 tunning.py:109] model.build()
I1201 01:19:38.768105 139641327941440 tunning.py:82] model.fit()
I1201 01:19:38.768769 139641327941440 tunning.py:83] run_id: 17
I1201 01:19:38.769261 139641327941440 tunning.py:84] epochs:2
I1201 01:19:38.770471 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:38.771028 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  0.6888809651136398
Average test accuracy:  0.53125
I1201 01:19:42.785322 139641327941440 tunning.py:106] Model 17 saved to model_path: /labs/models/experiment_4/model_17.h5
I1201 01:19:42.792404 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:42.793220 139641327941440 tunning.py:122] run_id: 17
I1201 01:19:42.793755 139641327941440 tunning.py:123] steps: 23
I1201 01:19:43.744130 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_17.h5
I1201 01:19:43.744838 139641327941440 layer_utils.py:165] Model: "sequential_17"
I1201 01:19:43.745426 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:43.746027 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:43.746680 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:43.747497 139641327941440 layer_utils.py:163] conv2d_39 (Conv2D)           (None, 224, 224, 8)       224       
I1201 01:19:43.748079 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.748709 139641327941440 layer_utils.py:163] max_pooling2d_39 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:43.749304 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.750039 139641327941440 layer_utils.py:163] conv2d_40 (Conv2D)           (None, 112, 112, 16)      1168      
I1201 01:19:43.751331 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.752052 139641327941440 layer_utils.py:163] max_pooling2d_40 (MaxPooling (None, 56, 56, 16)        0         
I1201 01:19:43.752687 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.753417 139641327941440 layer_utils.py:163] conv2d_41 (Conv2D)           (None, 56, 56, 32)        4640      
I1201 01:19:43.754029 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.754603 139641327941440 layer_utils.py:163] max_pooling2d_41 (MaxPooling (None, 28, 28, 32)        0         
I1201 01:19:43.755101 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.755631 139641327941440 layer_utils.py:163] flatten_17 (Flatten)         (None, 25088)             0         
I1201 01:19:43.756026 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.757187 139641327941440 layer_utils.py:163] dropout_17 (Dropout)         (None, 25088)             0         
I1201 01:19:43.757800 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.758488 139641327941440 layer_utils.py:163] dense_52 (Dense)             (None, 32)                802848    
I1201 01:19:43.759000 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.759737 139641327941440 layer_utils.py:163] dense_53 (Dense)             (None, 64)                2112      
I1201 01:19:43.760294 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.760914 139641327941440 layer_utils.py:163] dense_54 (Dense)             (None, 128)               8320      
I1201 01:19:43.761412 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:43.762093 139641327941440 layer_utils.py:163] dense_55 (Dense)             (None, 2)                 258       
I1201 01:19:43.762626 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:43.763719 139641327941440 layer_utils.py:242] Total params: 819,570
I1201 01:19:43.764179 139641327941440 layer_utils.py:243] Trainable params: 819,570
I1201 01:19:43.764740 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:43.765227 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:19:47.756851 139641327941440 tunning.py:60] --- Running training session 19, 20
I1201 01:19:47.757434 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.36114136963259647, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:19:47.757964 139641327941440 tunning.py:62] --- repeat #: 1
I1201 01:19:47.758963 139641327941440 keras_models.py:35] index: 0
I1201 01:19:47.884044 139641327941440 tunning.py:109] model.build()
I1201 01:19:47.922184 139641327941440 tunning.py:82] model.fit()
I1201 01:19:47.923090 139641327941440 tunning.py:83] run_id: 18
I1201 01:19:47.923758 139641327941440 tunning.py:84] epochs:2
I1201 01:19:47.924374 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:47.925017 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.4342627674341202
Average test accuracy:  0.5625
I1201 01:19:52.005428 139641327941440 tunning.py:106] Model 18 saved to model_path: /labs/models/experiment_4/model_18.h5
I1201 01:19:52.036959 139641327941440 tunning.py:121] model.evaluate()
I1201 01:19:52.037719 139641327941440 tunning.py:122] run_id: 18
I1201 01:19:52.038357 139641327941440 tunning.py:123] steps: 23
I1201 01:19:52.929541 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_18.h5
I1201 01:19:52.930306 139641327941440 layer_utils.py:165] Model: "sequential_18"
I1201 01:19:52.930985 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:19:52.931559 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:19:52.932183 139641327941440 layer_utils.py:168] =================================================================
I1201 01:19:52.933002 139641327941440 layer_utils.py:163] conv2d_42 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:19:52.933597 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.934166 139641327941440 layer_utils.py:163] max_pooling2d_42 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:19:52.934642 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.935275 139641327941440 layer_utils.py:163] flatten_18 (Flatten)         (None, 100352)            0         
I1201 01:19:52.935786 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.937160 139641327941440 layer_utils.py:163] dropout_18 (Dropout)         (None, 100352)            0         
I1201 01:19:52.937958 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.943057 139641327941440 layer_utils.py:163] dense_56 (Dense)             (None, 32)                3211296   
I1201 01:19:52.943830 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.944533 139641327941440 layer_utils.py:163] dense_57 (Dense)             (None, 64)                2112      
I1201 01:19:52.945185 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.945943 139641327941440 layer_utils.py:163] dense_58 (Dense)             (None, 128)               8320      
I1201 01:19:52.946498 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:19:52.947259 139641327941440 layer_utils.py:163] dense_59 (Dense)             (None, 2)                 258       
I1201 01:19:52.947911 139641327941440 layer_utils.py:230] =================================================================
I1201 01:19:52.949038 139641327941440 layer_utils.py:242] Total params: 3,222,594
I1201 01:19:52.949691 139641327941440 layer_utils.py:243] Trainable params: 3,222,594
I1201 01:19:52.950266 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:19:52.950754 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:19:57.130225 139641327941440 tunning.py:60] --- Running training session 20, 20
I1201 01:19:57.132591 139641327941440 tunning.py:61] {HParam(name='conv_layers', domain=IntInterval(1, 3), display_name=None, description=None): 1, HParam(name='conv_kernel_size', domain=Discrete([3, 5]), display_name=None, description=None): 5, HParam(name='dense_layers', domain=IntInterval(1, 3), display_name=None, description=None): 3, HParam(name='dropout', domain=RealInterval(0.1, 0.4), display_name=None, description=None): 0.36114136963259647, HParam(name='optimizer', domain=Discrete(['adagrad', 'adam']), display_name=None, description=None): 'adagrad'}
I1201 01:19:57.133406 139641327941440 tunning.py:62] --- repeat #: 2
I1201 01:19:57.134662 139641327941440 keras_models.py:35] index: 0
I1201 01:19:57.251659 139641327941440 tunning.py:109] model.build()
I1201 01:19:57.291470 139641327941440 tunning.py:82] model.fit()
I1201 01:19:57.292281 139641327941440 tunning.py:83] run_id: 19
I1201 01:19:57.292978 139641327941440 tunning.py:84] epochs:2
I1201 01:19:57.293632 139641327941440 tunning.py:85] steps_per_epoch:2
I1201 01:19:57.294238 139641327941440 tunning.py:86] validation_steps:1
Average test loss:  1.2217622995376587
Average test accuracy:  0.578125
I1201 01:20:01.866879 139641327941440 tunning.py:106] Model 19 saved to model_path: /labs/models/experiment_4/model_19.h5
I1201 01:20:01.902118 139641327941440 tunning.py:121] model.evaluate()
I1201 01:20:01.903131 139641327941440 tunning.py:122] run_id: 19
I1201 01:20:01.903751 139641327941440 tunning.py:123] steps: 23
I1201 01:20:02.796735 139641327941440 tunning.py:126] model was loaded from/labs/models/experiment_4/model_19.h5
I1201 01:20:02.797603 139641327941440 layer_utils.py:165] Model: "sequential_19"
I1201 01:20:02.798357 139641327941440 layer_utils.py:166] _________________________________________________________________
I1201 01:20:02.799049 139641327941440 layer_utils.py:163] Layer (type)                 Output Shape              Param #   
I1201 01:20:02.799694 139641327941440 layer_utils.py:168] =================================================================
I1201 01:20:02.800538 139641327941440 layer_utils.py:163] conv2d_43 (Conv2D)           (None, 224, 224, 8)       608       
I1201 01:20:02.801126 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.801711 139641327941440 layer_utils.py:163] max_pooling2d_43 (MaxPooling (None, 112, 112, 8)       0         
I1201 01:20:02.802218 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.802873 139641327941440 layer_utils.py:163] flatten_19 (Flatten)         (None, 100352)            0         
I1201 01:20:02.803369 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.803979 139641327941440 layer_utils.py:163] dropout_19 (Dropout)         (None, 100352)            0         
I1201 01:20:02.804494 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.805262 139641327941440 layer_utils.py:163] dense_60 (Dense)             (None, 32)                3211296   
I1201 01:20:02.805823 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.806499 139641327941440 layer_utils.py:163] dense_61 (Dense)             (None, 64)                2112      
I1201 01:20:02.807106 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.807751 139641327941440 layer_utils.py:163] dense_62 (Dense)             (None, 128)               8320      
I1201 01:20:02.808268 139641327941440 layer_utils.py:232] _________________________________________________________________
I1201 01:20:02.808918 139641327941440 layer_utils.py:163] dense_63 (Dense)             (None, 2)                 258       
I1201 01:20:02.809481 139641327941440 layer_utils.py:230] =================================================================
I1201 01:20:02.810473 139641327941440 layer_utils.py:242] Total params: 3,222,594
I1201 01:20:02.811142 139641327941440 layer_utils.py:243] Trainable params: 3,222,594
I1201 01:20:02.811670 139641327941440 layer_utils.py:244] Non-trainable params: 0
I1201 01:20:02.812125 139641327941440 layer_utils.py:245] _________________________________________________________________
accuracy: 0.5217391
I1201 01:20:07.007394 139641327941440 logger.py:42] Removing cache_file_dir: /labs/data/processed/experiment_4
I1201 01:20:07.052158 139641327941440 logger.py:44] LOGGING END. Data was saved to logs_dir: /labs/logs/experiment_4, models_dir: /labs/models/experiment_4
